{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b2ceb21-b968-49ff-af5d-b33513b35887",
   "metadata": {},
   "source": [
    "# Image Classification Model Comparison with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7545d7e5-edca-4aad-92e6-64634c54a71b",
   "metadata": {},
   "source": [
    "### Project Overview:\n",
    "##### The goal of this project is to build two models that can identify an image and what class it belongs to. We are comparing the performance between ResNet and Vision Transformers (ViT) architectures for image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f01a80-cf60-4796-9851-d8c642ecefc0",
   "metadata": {},
   "source": [
    "##### This repository contains a Jupyter Notebook that implements an image classification model using the ResNet18 architecture, as described in the research paper by He, Kaiming, et al. “Deep Residual Learning for Image Recognition.” arXiv, 10 Dec. 2015, arxiv.org/abs/1512.03385. \n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:720/format:webp/1*rrlou8xyh7DeWdtHZk_m4Q.png' width='800'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139d222a-13fb-4c49-8748-04f70cff5ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\n",
    "import opendatasets as od"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3632590-65d6-406c-b7ef-e4bb4bba72a6",
   "metadata": {},
   "source": [
    "### ResNet18 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9534fc-0e3e-477b-bcce-58df279a3496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "# device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3d48c0-a797-4c31-afc2-f3b0e2a7834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "od.download(\"https://www.kaggle.com/datasets/saumyamohandas/garbage-classification-image-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb42cf-3817-45a7-ac98-74e569041e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"garbage-classification-image-dataset/dataset/Training\"\n",
    "test_path = \"garbage-classification-image-dataset/dataset/Testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8aa1f6-a927-4846-b57c-b8943ce7725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "num_epochs = 11\n",
    "batch_size = 10\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f242cc0-5bb9-4552-a3e3-7e609ff320dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to tensors of normalized range [-1, 1]\n",
    "# Augment the data by performing a random horizontal flip to the data and a random crop\n",
    "\n",
    "transform_train = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                                     transforms.RandomHorizontalFlip(), transforms.Resize([256, 256])])\n",
    "\n",
    "transform_test = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), transforms.Resize([256, 256])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bdb0ff-f257-4744-a19d-5397bbd209f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "train_dataset = ImageFolder(root=train_path, transform=transform_train)\n",
    "\n",
    "test_dataset = ImageFolder(root=test_path, transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782633c0-5071-41df-a207-827337bba012",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_dataset.classes\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169a7037-3223-4b21-86d8-fcb3ef9a4731",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.resnet.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d462bcc-c962-435d-8eb1-9b12ab1cf289",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d83319-6365-42b9-b8be-715540305656",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in ResNet().named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ff9ae-2fd6-4259-b347-2253acc8769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #backward propogation and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Append loss\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Append loss\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "    # Log losses after epoch\n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "    avg_test_loss = np.mean(test_losses)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e7a394-01c0-4e7e-96e7-ee1463dc2e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(6)]\n",
    "    n_class_samples = [0 for i in range(6)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "                n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the networks: {acc} %')\n",
    "\n",
    "    for i in range(6):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(6)]\n",
    "    n_class_samples = [0 for i in range(6)]\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "acc = 100.0 * n_correct / n_samples\n",
    "print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "for i in range(6):\n",
    "    acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "    print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f217f-34fc-4f38-9081-a55c6f4ca0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = iter(test_loader)\n",
    "images, samples = examples.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e03f0-b641-4c33-b444-e3301792d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img/2 + 0.2\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cac2d83-fdc1-4467-bff8-ad0acf15e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(torchvision.utils.make_grid(images, nrow=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6149993-5569-4277-9d31-0f8d39364d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels for images\")\n",
    "print(samples)\n",
    "preds = _, predictions = torch.max(model(images.cuda()), 1)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Predicitons for Image {i+1}\")\n",
    "    print(classes[predictions[i]])\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Truth for Image {i+1}\")\n",
    "    print(classes[samples[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2361a704-e654-47c0-b523-af29b176eec5",
   "metadata": {},
   "source": [
    "### Vision Transformer Model (ViT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d450e47-d702-4dd4-83a7-4821d9959f1b",
   "metadata": {},
   "source": [
    "##### In addition, this notebook contains another image classification model using the Vision Transformer (ViT) architecture, as described in the research paper by Dosovitskiy, Alexey, et al. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” arXiv, 22 Oct. 2020, https://arxiv.org/abs/2010.11929. Accessed 5 July 2024. \n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:720/format:webp/1*SoXHGxDPUqFQHFbJKYoVLg.png' width='800'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b9a5b-e8fe-47c2-865a-df645df38208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "num_epochs = 11\n",
    "batch_size = 10\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262794d3-7fa0-4c1a-a350-7c23cd7e50f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to tensors of normalized range [-1, 1]\n",
    "# Augment the data by performing a random horizontal flip to the data and a random crop\n",
    "\n",
    "transform_train = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                                     transforms.RandomHorizontalFlip(), transforms.Resize([224, 224])])\n",
    "\n",
    "transform_test = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), transforms.Resize([224, 224])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b26e0-aa98-415a-a99d-0f7a52879b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "train_dataset = ImageFolder(root=train_path, transform=transform_train)\n",
    "\n",
    "test_dataset = ImageFolder(root=test_path, transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a999cc-a256-43a7-9032-99c4453cecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_dataset.classes\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f55c89-6171-495d-8321-c4b4d1e40604",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(ViT, self).__init__()\n",
    "        self.ViT = vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\n",
    "        for param in self.ViT.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.ViT.fc = nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ViT(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3a5549-547f-43a5-a7e1-8a9b56649e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd073f7-bff3-4c10-a7ec-428115bd49d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in ViT().named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e6d91-78b9-4b37-9a39-e0d938c68c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #backward propogation and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Append loss\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Append loss\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "    # Log losses after epoch\n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "    avg_test_loss = np.mean(test_losses)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be178d9-5793-4b35-af06-7e762a318f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(6)]\n",
    "    n_class_samples = [0 for i in range(6)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "                n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the networks: {acc} %')\n",
    "\n",
    "    for i in range(6):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(6)]\n",
    "    n_class_samples = [0 for i in range(6)]\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "acc = 100.0 * n_correct / n_samples\n",
    "print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "for i in range(6):\n",
    "    acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "    print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7659199-40d3-42ee-bd31-76d991a7f443",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = iter(test_loader)\n",
    "images, samples = examples.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db1a441-e3ac-4eca-a64b-ff11ee513393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img/2 + 0.2\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc018018-8f21-4862-8a3a-206fb16aaecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(torchvision.utils.make_grid(images, nrow=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae8c82-f230-4361-8c33-4c81fa12f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels for images\")\n",
    "print(samples)\n",
    "preds = _, predictions = torch.max(model(images.cuda()), 1)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Predicitons for Image {i+1}\")\n",
    "    print(classes[predictions[i]])\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Truth for Image {i+1}\")\n",
    "    print(classes[samples[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42acabfc-2092-4efa-89cd-a014392eb9a7",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "##### In conclusion, I recommend using ResNet models when training a machine to classify images. I think this because while it is known that ResNet models train faster and Vision Transformer models are known for their strong performance in certain scenarios, it does not offer a significant improvement in accuracy in this instance. Therefore, the quicker training time of ResNet makes it the more efficient choice without sacrificing substantial model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
